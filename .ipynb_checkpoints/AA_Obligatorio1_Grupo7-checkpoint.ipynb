{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "57qTLxksUMZk"
   },
   "source": [
    "# Entrega 1 - jugador de Teeko\n",
    "\n",
    "### Grupo 7:\n",
    "     - S. Agustina Sierra Lima C.I. 4.647.235-6\n",
    "     - V. Sebastian Volti Diano C.I 5.175.914-7\n",
    "     - C. Alejandro Clara C.I. 4.772.294-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7PaXyYfUUMZr"
   },
   "source": [
    "## 1. Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjFXRcPiUMZv"
   },
   "source": [
    "El objetivo de esta tarea es construir un jugador de Teeko utilizando t√©cnicas de aprendizaje autom√°tico.\n",
    "En el marco de la definici√≥n de aprendizaje autom√°tico identificamos las tres variables $T$, $P$ y $E$ en nuestro dise√±o de la siguiente manera:\n",
    "\n",
    "- Deseamos mejorar en la tarea $T$: Jugar al Teeko.\n",
    "\n",
    "- Respecto a la siguiente medida $P$: % de juegos ganados.\n",
    "\n",
    "- Bas√°ndonos en la experiencia $E$:\n",
    "\n",
    "  i) Inicialmente el algoritmo aprende jugando contra un oponente que juega de forma aleatoria.\n",
    "  \n",
    "  ii) Luego se usa como oponente una versi√≥n aprendida en alguna iteraci√≥n anterior.\n",
    "\n",
    "Para representar lo que efectivamente queremos aprender hemos definido ciertas funciones:  \n",
    "  \n",
    "$ChooseMove:tablero\\rightarrow movimiento$, la cual toma un tablero como entrada y su salida es un movimiento del conjunto de movimientos posibles que un jugador puede hacer.\n",
    "\n",
    "$V:tablero\\rightarrow \\mathbb{R}$, donde $V$ asigna los valores m√°s altos a los mejores tableros, es decir, a los que tienen mayores chances de llevar a ganar la partida.\n",
    "\n",
    "De esta forma hemos traducido el problema de mejorar el rendimiento $P$ de una tarea $T$, al problema de encontrar la funci√≥n $ChooseMove$ tal que, dado un tablero devuelve el mejor movimiento posible para un jugador. Es decir, el movimiento que lleve al tablero con valor m√°s alto seg√∫n $V$, dentro de los movimientos posibles.\n",
    "\n",
    "Tal y como vimos en clase la funci√≥n $V$ no es efectivamente computable, lo que la hace no operativa.\n",
    "El objetivo que nos proponemos es encontrar la mejor aproximaci√≥n a tal funci√≥n.\n",
    "\n",
    "Entonces definimos:  \n",
    "  \n",
    "$V_{op}:tablero\\rightarrow \\mathbb{R}$\n",
    "Donde $V_{op}$ la definimos como una serie de caracter√≠sticas que representan el estado del tablero y adem√°s cumple que:  \n",
    "  \n",
    "$V_{op}(t_{ganador})=1$  \n",
    "$V_{op}(t_{perdedor})=-1$  \n",
    "$V_{op}(t_{empate})=0$\n",
    "\n",
    "\n",
    "$V_{op}(t)=w_{0}+w_{1}X_{1}+w_{2}X_{2}+w_{3}X_{3}+w_{4}X_{4}+w_{5}X_{5}+w_{6}X_{6}+w_{7}X_{7}+w_{8}X_{8}$  \n",
    "  \n",
    "Donde:    \n",
    "$X_{1}=$ Cantidad de adyacencias horizontales del jugador 1   \n",
    "$X_{2}=$ Cantidad de adyacencias vertivales del jugador 1  \n",
    "$X_{3}=$ Cantidad de adyacencias diagonales vistas de izquierda (arriba) a derecha (abajo) del jugador 1   \n",
    "$X_{4}=$ Cantidad de adyacencias diagonales vistas de derecha (arriba) a izquierda (abajo) del jugador 1   \n",
    "$X_{5}=$ Cantidad de adyacencias horizontales del jugador 2   \n",
    "$X_{6}=$ Cantidad de adyacencias verticales del jugador 2  \n",
    "$X_{7}=$ Cantidad de adyacencias diagonales vistas de izquierda (arriba) a derecha (abajo) del jugador 2  \n",
    "$X_{8}=$ Cantidad de adyacencias diagonales vistas de derecha (arriba) a izquierda (abajo) del jugador 2   \n",
    "  \n",
    "En conclusi√≥n, nos proponemos encontrar los valores $w_{i}$ √≥ptimos para aproximar $V_{op}$ a $V$.\n",
    "Utilizamos la t√©cnica de m√≠nimos cuadrados vista en clase, variando algunos par√°metros que veremos m√°s adelante en el informe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZaxbuHUUMZx"
   },
   "source": [
    "## 2. Dise√±o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri2E2GL1UMZ3"
   },
   "source": [
    "## 2.1 Juego\n",
    "\n",
    "- El juego se compone de un tablero de 5x5 y cuatro fichas para cada jugador. El objetivo es lograr tener cuatro fichas en l√≠nea recta (vertical, horizontal o diagonal) o en un cuadrado de cuatro espacios adyacentes.\n",
    "- Una de las simplificaciones especificadas en la letra es que se comienza con un tablero con todas sus fichas posicionadas y solo resta moverlas. Para elegir tales tableros iniciales utilizamos una funci√≥n que los genera aleatoriamente.\n",
    "- Existen fichas negras y rojas, dado que las reglas indican que siempre comienza el jugador con las fichas negras diremos que el jugador que nos interesa entrenar (el jugador 1) es el que juega primero, es decir, el que tiene las fichas negras. Las fichas rojas corresponder√°n al oponente (el jugador 2).\n",
    "- Definimos como empate si se dan mas de 500 movimientos en total y a√∫n ning√∫n jugador ha ganado.\n",
    "\n",
    "\n",
    "## 2.2 Tablero\n",
    "\n",
    "- Representamos un tablero como una matriz de 5x5 en donde colocamos 1's en las coordenadas que el jugador 1 tiene sus fichas y 2's en las coordenadas donde el jugador 2 tiene sus fichas. El resto de los lugares libres se representan con 0's.\n",
    "- Luego de haber le√≠do el ejemplo del juego de damas quisimos trazar una similitud con el juego en cuesti√≥n. Creemos que las caracter√≠sticas del tablero que elegimos son las m√°s representativas del estado del juego, en el sentido de que con solo listarlas nos da una idea de qu√© jugador est√° m√°s cerca de ganar y as√≠ poder llegar a una forma correcta de dar valores a los diferentes estados de los tableros. \n",
    "A continuaci√≥n, una serie de ejemplos:  \n",
    "  \n",
    "$\\begin{pmatrix}\n",
    "0 & 1 & 1 & 1 & 1 \\\\\n",
    "0 & 2 & 0 & 0 & 2 \\\\\n",
    "0 & 2 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 2 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$ $X_{1}=$ 3, $X_{2}=$ 0, $X_{3}=$ 0, $X_{4}=$ 0, $X_{5}=$ 0, $X_{6}=$ 1, $X_{7}=$ 1, $X_{8}=$ 0  \n",
    "\n",
    "$\\begin{pmatrix}\n",
    "0 & 0 & 1 & 0 & 1 \\\\\n",
    "0 & 2 & 2 & 0 & 1 \\\\\n",
    "0 & 2 & 2 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$  $X_{1}=$ 0, $X_{2}=$ 2, $X_{3}=$ 0, $X_{4}=$ 0, $X_{5}=$ 2, $X_{6}=$ 2, $X_{7}=$ 1, $X_{8}=$ 1\n",
    "\n",
    "$\\begin{pmatrix}  \n",
    "1 & 0 & 2 & 2 & 2 \\\\\n",
    "0 & 1 & 0 & 0 & 2 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$ $X_{1}=$ 0, $X_{2}=$ 0, $X_{3}=$ 3, $X_{4}=$ 0, $X_{5}=$ 2, $X_{6}=$ 1, $X_{7}=$ 1, $X_{8}=$ 0  \n",
    "  \n",
    "Estos son ejemplos de tableros finales. En t√©rminos generales sabemos que tenemos tableros finales si:\n",
    "- Las adyacencias horizontales son 3 √≥\n",
    "- Las adyacencias verticales son 3 √≥\n",
    "- Las adyacencias diagonales de izquierda (arriba) a derecha (abajo) son 3 √≥\n",
    "- Las adyacencias diagonales de derecha (arriba) a izquierda (abajo) son 3 √≥\n",
    "- Las adyacencias horizontales son 2 y las adyacencias verticales son 2\n",
    "\n",
    "\n",
    "## 2.3 Algoritmo\n",
    "\n",
    "### 2.3.1 Visi√≥n general\n",
    "\n",
    "Siguiendo la idea planteada en el libro, dividimos el algoritmo en 4 m√≥dulos: _Performance System, Critic, Generalizer y Generator_.\n",
    "\n",
    "**Performance System**: Utilizando las funciones aprendidas, es el que se encarga de realizar una partida. Toma como entrada un tablero inicial y su salida corresponde a un historial de juego.\n",
    "Definimos un historial como todos los tableros por los que pasa el jugador 1 luego de que realiza un movimiento.  \n",
    "  \n",
    "**Critic**: Se encarga de producir los ejemplos de entrenamiento. Toma como entrada un historial de juego y su salida es un conjunto de tuplas $<t,V_{train}(t)>$, donde $V_{train}(t)$ es un valor aproximado que calculamos de la siguiente manera:\n",
    "  \n",
    "  $V_{train}(t_{intermedio}) ‚Üê V_{op}(t'_{siguiente\\space tablero\\space en\\space el\\space historial})$\n",
    "  \n",
    "  \n",
    "**Generalizer**: Toma como entrada los ejemplos de entrenamiento y su salida es una nueva aproximaci√≥n de la funci√≥n objetivo, luego de haber aplicado el algoritmo de m√≠nimos cuadrados. Aqu√≠ es donde se ajustan los valores $w_{i}$. El algoritmo de ajuste que utilizamos es el que se dio en clase.\n",
    "  \n",
    "**Generator**: Es el encargado tanto de generar un nuevo tablero inicial, as√≠ como de configurar todos los par√°metros necesarios para una nueva partida. Toma como par√°metro una funci√≥n $V_{op}$ y su salida es un nuevo tablero inicial.\n",
    "\n",
    "Definimos una **iteraci√≥n** como una vuelta completa sobre cada uno de los m√≥dulos definidos arriba.\n",
    "\n",
    "### 2.3.2 Ajuste de la funci√≥n\n",
    "\n",
    "El orden para completar un ajuste de la funci√≥n objetivo ser√≠a: _Generator -> Performance System -> Critic -> Generalizer_ y el ajuste se realiza en el m√≥dulo _Generalizer_.\n",
    "\n",
    "### 2.3.3 Parametrizaci√≥n\n",
    "\n",
    "A nivel de implementaci√≥n, decidimos dise√±ar el c√≥digo de manera tal que nos resultara f√°cil ajustar los par√°metros para el momento de realizar la experimentaci√≥n. En dicha secci√≥n se prueba con varios valores de Œº, en algunos casos se prueba con enfriamiento, se prueba con un factor de descuento, etc.\n",
    "\n",
    "## 2.4 Contrarios\n",
    "\n",
    "- Para generar los tableros al azar se genera una matriz de 5x5 con valores todos en 0. Luego, para poblar el tablero, para cada jugador se generan tuplas que representan las coordenadas de d√≥nde va a ser colocada la ficha. En ambas coordenadas se usa la funci√≥n de Python randint(), hasta que se encuentra un lugar libre y se coloca all√≠ la ficha.\n",
    "- Para generar los movimientos al azar se generan todos los posibles siguientes tableros, se los guarda en una lista _L_ y se usa la funci√≥n de Python choice sobre _L_, la cual retorna un elemento al azar.\n",
    "- Los $w_{i}$ se actualizan en cada iteraci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbGVdjwFUMZ8"
   },
   "source": [
    "## 3. Experimentaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dw_7cqyfUMZ-"
   },
   "source": [
    "## 3.1 Oponente Aleatorio \n",
    "\n",
    "En esta primera parte mostraremos los resultados obtenidos de jugar con un oponente que siempre juegue de forma aleatoria.\n",
    "En cada punto se jug√≥ un total de 120 partidas, ajustando los $w_{i}$ luego de cada partida.\n",
    "\n",
    "### 3.1.1 $w_{i}$ iniciales\n",
    "\n",
    "Siempre que se comienza a entrenar, se realiza un entrenamiento inicial con tableros finales valorados con 1, 0 o -1. Una vez generado este conjunto de tableros finales creamos un historial y llamamos a los m√≥dulos _Critic_ y luego a _Generalizer_. As√≠, ajustamos tantas veces como tableros finales haya en el conjunto. Finalmente, con la funci√≥n $V$ y con los  $w_{i}$ conseguidos iniciamos el entrenamiento.\n",
    "     \n",
    "Probamos con tres vectores iniciales (pre-entrenamiento inicial) distintos y nos quedamos con el vector de $w_{i}$ que nos dio un porcentaje m√°s alto de partidas ganadas.\n",
    "Dichos vectores fueron los siguientes:\n",
    "\n",
    "1. (0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25)\n",
    "2. (0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75)\n",
    "3. (0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45)\n",
    "    \n",
    "  Resultados:  \n",
    "<table>\n",
    "  <tr style=\"font-weight:bold\">\n",
    "    <th>(w0,w1,w2,w3,w4,w5,w6,w7,w8)</th>\n",
    "    <th>Partidas ganadas</th>\n",
    "    <th>% de partidas ganadas</th>\n",
    "    <th>% de partidas empatadas</th>\n",
    "  </tr>   \n",
    "  <tr>\n",
    "    <td>(0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25)</td>\n",
    "    <td>91</td>\n",
    "    <td>50.5</td>\n",
    "    <td>13.3</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75)</td>\n",
    "    <td>76</td>\n",
    "    <td>42.2</td>\n",
    "    <td>15.6</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45)</td>\n",
    "    <td>106</td>\n",
    "    <td>58.8</td>\n",
    "    <td>13.8</td>\n",
    "  </tr>    \n",
    "</table>\n",
    "\n",
    "En conclusi√≥n, nos quedamos con el vector inicial $W$ igual a 0.45 en todas sus entradas.\n",
    "  \n",
    "### 3.1.2 Elecci√≥n del pr√≥ximo movimiento\n",
    "En todos los experimentos que realizamos en esta secci√≥n, el jugador dado el conjunto de todos los movimientos posibles elige el que tiene una valoraci√≥n mayor seg√∫n la funci√≥n objetivo $V$ que conoce hasta el momento.  \n",
    "\n",
    "### 3.1.3 Aproximaci√≥n para $ùëâ_{ùë°ùëüùëéùëñùëõ}(ùë°)$\n",
    "\n",
    "Si bien al principio la idea fue mapear el espacio de los posibles tableros a un entorno (-1, 1) donde los tableros ganadores tomaran el valor 1 y los perdedores el valor -1, esto no se pudo lograr de forma estricta, dado que los $w_{i}$ tend√≠an a crecer y las caracter√≠sticas que decidimos representar de los tableros pod√≠an llegar a ser n√∫meros que hicieran crecer la funci√≥n, haciendo que tomara valores m√°s grandes que 1.\n",
    "\n",
    "Adem√°s, de alguna manera, imponer que los tableros finales tuvieran valores -1 o 1 despistaba al algoritmo. Creemos que puede haber sido por tener tableros anteriores con valores m√°s (menos) grandes que un tablero final ganador (perdedor). Lo que hicimos para enfrentar este problema fue directamente descartar los tableros finales al momento de generar el conjunto de entrenamiento. \n",
    "\n",
    "Otra opci√≥n podr√≠a haber sido utilizar la noci√≥n de que dado un $t\\space/\\space t\\space es \\space final =>$ $V_{train}(t)=V_{op}(t)$ logrando as√≠ mantener la idea de que los tableros con valores mas grandes son mejores.\n",
    "\n",
    "### 3.1.4 Movimientos aleatorios\n",
    "Uno de los fen√≥menos que observamos es que luego de muchos entrenamientos el algoritmo se estancaba y no exporaba nuevos tableros. Decidimos probar tomando cada X movimientos un movimiento aleatorio y ver qu√© suced√≠a con el porcentaje de partidas gandas. Este fue el resultado:\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Cada 2 movimientos uno aleatorio</th>\n",
    "    <th>Cada 3 movimientos uno aleatorio</th>\n",
    "    <th>Cada 4 movimientos uno aleatorio</th>\n",
    "    <th>Cada 5 movimientos uno aleatorio</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>77.7%</td>\n",
    "    <td>87.6%</td>\n",
    "    <td>80.5%</td>\n",
    "    <td>83.3%</td>\n",
    "  </tr>    \n",
    "</table>\n",
    "  \n",
    "  En conclusi√≥n, nos quedaremos con la configuraci√≥n que nos dio un total de 86.6% de partidas ganadas. Es decir, elegir un movimiento aleatorio cada 3 √≤ 5 movimientos (contando solo movimientos del Jugador que est√° siendo entrenado).\n",
    "  \n",
    "  _**Observaci√≥n:** Notamos que luego de fijar esta variante no tenemos empates._\n",
    "  \n",
    "### 3.1.5 Tasa de aprendizaje Œº y enfriamiento\n",
    "A nivel conceptual, a mayor valor de Œº, mayor es el ajuste. Fuimos probando con distintos valores de Œº en cada entrenamiento, y los resultados fueron los siguientes:\n",
    "    \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Œº</th>\n",
    "    <th>% partidas ganadas</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Œº=0.1</td>\n",
    "    <td>91.3</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>Œº=0.01</td>\n",
    "    <td>94.4</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>Œº=0.001</td>\n",
    "    <td>95.6</td>\n",
    "  </tr>  \n",
    "</table>\n",
    "\n",
    "En un intento por mejorar a√∫n m√°s el porcentaje probamos con distintas tasas de enfriamiento, pero no vimos cambios notorios en los resultados finales.\n",
    "\n",
    "En conclusi√≥n, nos quedamos con Œº fijo igual a 0.001.\n",
    "\n",
    "\n",
    "### 3.1.6 Factor de descuento $\\gamma$\n",
    "Definimos un factor de descuento igual a $(0.9)^k$ con el objetivo de dar una menor valoraci√≥n a los tableros que estan m√°s cerca del tablero inicial.\n",
    "\n",
    "El porcentaje de partidas ganadas sin el factor de descuento fue de 95.6%.\n",
    "\n",
    "Con el factor de descuento fue de 96.4%, por lo que decidimos conservarlo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gt0G9k9UMaB"
   },
   "source": [
    "## 3.2 Oponente inteligente\n",
    "\n",
    "En esta secci√≥n veremos los resultados obtenidos al jugar contra un oponente que es una version anterior del jugador 1.\n",
    "\n",
    "### 3.2.1 Oponente con pesos fijos\n",
    " \n",
    "Primero probamos entrenar contra un jugador 2 que utiliza la funci√≥n objetivo $V_{op}$ parte 3.1 y nunca la actualiza:\n",
    "\n",
    "Aqu√≠ notamos que con los hiperpar√°mentros utilizados en la parte 3.1 el jugador 1 sigue teniendo mejor porcentaje de partidas ganadas. Pero para estar seguros que no existia una mejor combinacion de hiperparametros y variables realizamos el siguiente experimento:\n",
    "- Aumentamos la cantidad de partidas jugadas a 1000.   \n",
    "- Probamos con distintos factores de aprendizaje.  \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Œº = 0.1</th>\n",
    "    <th>Œº = 0.01</th>\n",
    "    <th>Œº = 0.001</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>jugador 1 43.3% / juegador 2  37.8%  </td>\n",
    "    <td>jugador 1 49.6% / juegador 2 34.6</td>\n",
    "    <td>jugador 1 72.7% / juegador 2 27.3%</td>  \n",
    "  </tr>\n",
    "     <caption>Esta tabla representa el porcentaje de partidas ganadas de cada jugador  jugador 1/jugador 2 variando Œº </caption>\n",
    "</table> \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QMMPschVUMaH"
   },
   "source": [
    "### 3.2.2 Oponente actualizado\n",
    "\n",
    "Luego, tomamos la decisi√≥n de ir variando los pesos del jugador 2 cada vez que transcurr√≠an $x$ partidas. Los valores $x$ que probamos son los siguientes:\n",
    "1. 50 partidas\n",
    "2. 20 partidas\n",
    "3. 5 partidas\n",
    "4. 2 partidas\n",
    "\n",
    "Utilizamos un factor de Œº = 0.001 para todos los casos, e inicializamos los pesos de ambos jugadores en 0.45 en todas sus entradas.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Cada 50 partidas</th>\n",
    "    <th>Cada 20 partidas</th>\n",
    "    <th>Cada 5 partidas</th>\n",
    "    <th>Cada 2 partidas</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>70.0% / 30.0% </td>\n",
    "    <td>70.5% / 29.5%</td>\n",
    "    <td>69.1% / 30.9%</td>\n",
    "    <td>68.2% / 31.8%</td>  \n",
    "  </tr>\n",
    "     <caption>Esta tabla muestra el porcentaje de partidas ganadas de jugador 1/jugador 2 variando $x$ </caption>\n",
    "</table>\n",
    "\n",
    "**Observaci√≥n:** No hubieron empates en esta parte.\n",
    "\n",
    "Se puede ver que no hubo un cambio significativo a medida que variamos $x$ si comparamos los porcentajes de ambos jugadores.\n",
    "Inicialmente supon√≠amos que a medida que el Jugador menos inteligente, en este caso el Jugador 2, actualizaba sus pesos m√°s frecuentemente, ibamos a observer cambios en los porcentajes, y obtener mejoras en la cantidad de partidas ganadas por el Jugador 2.\n",
    "Pero asumimos que el Jugador 1, al estar entrenando de forma continua, siempre se encuentra en mejores condiciones para llevarse la partida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GBheaaF8UMaI"
   },
   "source": [
    "## 3.3 Competencia\n",
    "\n",
    "En esta secci√≥n, la idea es que compitan la versi√≥n del jugador 1 entrenado en la primer parte de la tarea, vs la versi√≥n del jugador 1 entrenado en la segunda parte, esto es el jugador que entren√≥ vs un oponente que accionaba de forma aleatoria, compitiendo frente a el jugador que entren√≥ vs una versi√≥n anterior de √©l mismo.\n",
    "\n",
    "Llamaremos Jugador-Random al que entren√≥ vs un oponente random, y Jugador-Inteligente al restante.\n",
    "\n",
    "Como vimos en partes anteriores, obtuvimos mejores dividendos para el Jugador-Random cuando se jugaba con un mu peque√±o, por ejemplo con mu = 0.00001. \n",
    "Para el Jugador-Inteligente, la idea es escoger al jugador que entren√≥ vs el mejor oponente posible.\n",
    "\n",
    "Dividiremos esta secci√≥n en 3 competencias distintas.\n",
    "\n",
    "### 3.3.1 Competencia 1:\n",
    "- Tomaremos los pesos de la parte 1, resultantes de entrenar al Jugador-Random con pesos iniciales en 0.45 y Œº = 0.00001, donde se alcanzaban aproximadamente 95% de partidas ganadas.\n",
    "Los mismos son:   \n",
    "   \n",
    "                                  [0.308,0.394,0.394,0.408,0.408,0.430,0.429,0.433,0.434]\n",
    "Consideramos estos pesos los m√°s \"exigentes\" contra los que podr√≠a competir el Jugador-Inteligente entrenado en la segunda parte.\n",
    "\n",
    "- Partiendo de los mismos pesos iniciales (con todas sus coordenadas con el valor 0.45) y el mismo valor de Œº (0.00001), los pesos resultantes de entrenar al jugador de la parte 2 contra un oponente inteligente (que era la versi√≥n del jugador pero 5 partidas atr√°s) fueron los siguientes:   \n",
    "   \n",
    "                                 [0.298,0.403,0.403,0.411,0.411,0.387,0.386,0.401,0.401].\n",
    "\n",
    "- Obtuvimos los siguientes resultados al efectuar esta competencia:\n",
    "Jugador-Random gan√≥ el 22.20% del total de las 1000 partidas.\n",
    "Jugador-Inteligente gan√≥ el 24.59% del total de las 1000 partidas.\n",
    "Las partidas no finalizaron con un ganador en el 53.2% de las partidas.  \n",
    "  \n",
    "    \n",
    "  \n",
    "    \n",
    "Para no quedarnos solamente con los datos anteriores, probamos utilizar otras combinaciones de Jugador-Random/Jugador-Inteligente.\n",
    "En este caso, fijamos los pesos del Jugador-Random:    \n",
    " \n",
    "                                    [-0.256,0.082,0.082,0.137,0.124,0.207,0.203,0.224,0.230]. \n",
    "\n",
    "Estos son los resultantes de hacerlo competir en la primer parte de la tarea con un valor de Œº  un poco mas grande, 0.001.\n",
    "\n",
    "###3.3.2 Competencia 2:\n",
    "\n",
    "- Probamos con 2 \"jugadores inteligentes\" generados en la segunda parte de esta tarea (siempre partiendo de los mismos pesos iniciales 0.45, y jugando contra una versi√≥n 5 partidas atrasada).\n",
    "\n",
    " Para el primer Jugador-Inteligente utilizamos los mismos valores que en la competencia 1, estos son:  \n",
    "                         [0.298,0.403,0.403,0.411,0.411,0.387,0.386,0.401,0.401].\n",
    "\n",
    "- El resultado de esta competencia fue el siguiente:\n",
    "\n",
    " Jugador-Random gan√≥ el 21.6% del total de las 1000 partidas.\n",
    "Jugador-Inteligente gan√≥ el 25.7% del total de las 1000 partidas.\n",
    "Las partidas no finalizaron con un ganador en el 52.7% de las partidas.\n",
    "\n",
    "  Aqu√≠ notamos que el Jugador-Inteligente obtiene mejores resultados que en la primer competencia. \n",
    "\n",
    "##3.3.3 Competencia 3:\n",
    "\n",
    "- Para el segundo Jugador-Inteligente utilizamos mu = 0.001, obteniendo los siguientes pesos finales entrenado vs su versi√≥n anterior:  \n",
    "\n",
    "                        [-0.291,0.085,0.085,0.149,0.129,0.077,0.077,0.115,0.121]\n",
    "\n",
    "- Al realizar nuevamente la competencia, los resultados obtenidos fueron un poco inferiores:\n",
    "\n",
    "  Jugador-Random gan√≥ el 21.7% del total de las 1000 partidas.\n",
    "Jugador-Inteligente gan√≥ el 24.5% del total de las 1000 partidas.\n",
    "Las partidas no finalizaron con un ganador en el 53.8% de las partidas.  \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "Analizando los resultados obtenidos, podemos decir que si bien la comptenecia es muy pareja, se encuentra mejor entrenado el Jugador-Inteligente, y consigue ganar m√°s partidas que su oponente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vq9y7P9JUMaU"
   },
   "source": [
    "## 4. Conclusi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVpx88G8UMaV"
   },
   "source": [
    "- Los mejores resultados l√≥gicamente se dieron cuando jugabamos con un oponente aleatorio, dado que la inteligencia de tal era nula. A medida que el oponente se volv√≠a m√°s inteligente el porcentaje de partidas ganadas decreci√≥ notoriamente. En este punto tambi√©n observamos que con un oponente aleatorio casi no hab√≠an empates.\n",
    "- Otro punto a destacar, cuando oponente es aleatorio el hecho de que usar enfriamiento en la tasa de aprendizaje casi no cambi√≥ los resultados, con un Œº =0.001 que es bastante peque√±o obtuvimos buenos resultados. Lo que significa que si desde un principio limit√°bamos la variaci√≥n de el vector $W$ pod√≠amos converger a valores satisfactorios. Esto ocurre tambi√©n con esto cuando el oponente es una versi√≥n anterior del jugador 1.\n",
    "- Tambi√©n notamos que cuando entrenamos contra un oponente random, en un momento el porcentaje de partidas ganadas se estanca a pesar de jugar m√°s partidas. Esto pudimos solucionarlo haciendo que el jugador 1 cada cierta cantidad de movimientos realice uno aleatorio, permitiendo as√≠ que explore nuevos caminos y redujimos mucho m√°s los empates.\n",
    "En el caso de el oponente aleatorio reducimos a cero los empates pero en el caso del oponente inteligente no fue as√≠, con valores m√°s altos de Œº volv√≠an a aparecer los empates. Con lo que concluimos que es la combinaci√≥n de un factor de aprendizaje peque√±o m√°s los movimientos aleatorios son los que realmente hacen decrecer los empates.\n",
    "- Algo que creemos que mejorar√≠a el algoritmo es poder normalizar los atributos y los $w_{i}$ de forma tal de poder lograr que la imagen de la funci√≥n $V_{op}$ realemnte se encuentre en el intervalos $[-1,1]$. Logrando as√≠ poder darle valores conocidos a tableros finales. \n",
    "  Inicialmente creimos conveniente, para tener una idea de qu√© valores $w_{i}$ para comenzar a jugar pod√≠an ser √∫tiles, crear historiales de juego que constaran de solo un tablero final, dado que en principio $V_{train}$ para esos tableros era conocida, pero con el avance de la tarea nos dimos cuenta que era una falacia dado que existian tableros intemedios con un valor de $V_{op}$ mayor que 1.  \n",
    "- Tambi√©n podr√≠a mejorar el algoritmo cambiar la forma en la que elegimos el pr√≥ximo movimiento. La forma que nosotros aplicamos si bien nos fue √∫til, no creemos que sea la mejor. Una alternativa a esto es dado el conjunto de movimientos posibles para el jugador 1 si suponemos que nuestro oponente se mueve de forma √≥ptima (segun la $V_{op}$ conocida hasta el momento) podr√≠amos quedarnos con el movimiento que lleve al oponente al tablero menor valorado para √©l.\n",
    "- Algo que creemos que tambi√©n mejorar√≠a el algoritmo es cambiar agunas de las caracter√≠sticas que elegimos representar en el tablero, dado que podemos tener 3 adyacencias diagonales d√°ndole un valor alto a su $X_{i}$  correspondiente. Pero si las fichas estan en una diagonal de 3 el jugador nunca podr√° completar esta diagonal y ganar. Un ejemplo de tablero es el siguiente:  \n",
    "$\\begin{pmatrix}  \n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$. \n",
    "  \n",
    "  Una soluci√≥n es chequear en qu√© coordenadas de la matriz se est√° formando la diagonal para no permitir que el jugador que nos interesa entrenar coloque sus fichas de esta forma.\n",
    "\n",
    "- Otro aporte que podr√≠a ser de utilidad para que nuestro algoritmo mejore, adem√°s de fijarnos en el valor de nuestros atributos, ser√≠a verificar si el oponente tiene posibilidad de ganar en su pr√≥ximo movimiento (por tener por ejemplo 3 fichas adyacentes) y entonces colocar una de nuestras fichas en ese lugar, de manera de impedir su victoria."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AA_Obligatorio1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
