{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "57qTLxksUMZk"
   },
   "source": [
    "# Entrega 1 - jugador de Teeko\n",
    "\n",
    "### Grupo 7:\n",
    "     - S. Agustina Sierra Lima C.I. 4.647.235-6\n",
    "     - V. Sebastian Volti Diano C.I 5.175.914-7\n",
    "     - C. Alejandro Clara C.I. 4.772.294-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7PaXyYfUUMZr"
   },
   "source": [
    "## 1. Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjFXRcPiUMZv"
   },
   "source": [
    "El objetivo de esta tarea es construir un jugador de Teeko utilizando técnicas de aprendizaje automático.\n",
    "En el marco de la definición de aprendizaje automático identificamos las tres variables $T$, $P$ y $E$ en nuestro diseño de la siguiente manera:\n",
    "\n",
    "- Deseamos mejorar en la tarea $T$: Jugar al Teeko.\n",
    "\n",
    "- Respecto a la siguiente medida $P$: % de juegos ganados.\n",
    "\n",
    "- Basándonos en la experiencia $E$:\n",
    "\n",
    "  i) Inicialmente el algoritmo aprende jugando contra un oponente que juega de forma aleatoria.\n",
    "  \n",
    "  ii) Luego se usa como oponente una versión aprendida en alguna iteración anterior.\n",
    "\n",
    "Para representar lo que efectivamente queremos aprender hemos definido ciertas funciones:  \n",
    "  \n",
    "$ChooseMove:tablero\\rightarrow movimiento$, la cual toma un tablero como entrada y su salida es un movimiento del conjunto de movimientos posibles que un jugador puede hacer.\n",
    "\n",
    "$V:tablero\\rightarrow \\mathbb{R}$, donde $V$ asigna los valores más altos a los mejores tableros, es decir, a los que tienen mayores chances de llevar a ganar la partida.\n",
    "\n",
    "De esta forma hemos traducido el problema de mejorar el rendimiento $P$ de una tarea $T$, al problema de encontrar la función $ChooseMove$ tal que, dado un tablero devuelve el mejor movimiento posible para un jugador. Es decir, el movimiento que lleve al tablero con valor más alto según $V$, dentro de los movimientos posibles.\n",
    "\n",
    "Tal y como vimos en clase la función $V$ no es efectivamente computable, lo que la hace no operativa.\n",
    "El objetivo que nos proponemos es encontrar la mejor aproximación a tal función.\n",
    "\n",
    "Entonces definimos:  \n",
    "  \n",
    "$V_{op}:tablero\\rightarrow \\mathbb{R}$\n",
    "Donde $V_{op}$ la definimos como una serie de características que representan el estado del tablero y además cumple que:  \n",
    "  \n",
    "$V_{op}(t_{ganador})=1$  \n",
    "$V_{op}(t_{perdedor})=-1$  \n",
    "$V_{op}(t_{empate})=0$\n",
    "\n",
    "\n",
    "$V_{op}(t)=w_{0}+w_{1}X_{1}+w_{2}X_{2}+w_{3}X_{3}+w_{4}X_{4}+w_{5}X_{5}+w_{6}X_{6}+w_{7}X_{7}+w_{8}X_{8}$  \n",
    "  \n",
    "Donde:    \n",
    "$X_{1}=$ Cantidad de adyacencias horizontales del jugador 1   \n",
    "$X_{2}=$ Cantidad de adyacencias vertivales del jugador 1  \n",
    "$X_{3}=$ Cantidad de adyacencias diagonales vistas de izquierda (arriba) a derecha (abajo) del jugador 1   \n",
    "$X_{4}=$ Cantidad de adyacencias diagonales vistas de derecha (arriba) a izquierda (abajo) del jugador 1   \n",
    "$X_{5}=$ Cantidad de adyacencias horizontales del jugador 2   \n",
    "$X_{6}=$ Cantidad de adyacencias verticales del jugador 2  \n",
    "$X_{7}=$ Cantidad de adyacencias diagonales vistas de izquierda (arriba) a derecha (abajo) del jugador 2  \n",
    "$X_{8}=$ Cantidad de adyacencias diagonales vistas de derecha (arriba) a izquierda (abajo) del jugador 2   \n",
    "  \n",
    "En conclusión, nos proponemos encontrar los valores $w_{i}$ óptimos para aproximar $V_{op}$ a $V$.\n",
    "Utilizamos la técnica de mínimos cuadrados vista en clase, variando algunos parámetros que veremos más adelante en el informe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZaxbuHUUMZx"
   },
   "source": [
    "## 2. Diseño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri2E2GL1UMZ3"
   },
   "source": [
    "## 2.1 Juego\n",
    "\n",
    "- El juego se compone de un tablero de 5x5 y cuatro fichas para cada jugador. El objetivo es lograr tener cuatro fichas en línea recta (vertical, horizontal o diagonal) o en un cuadrado de cuatro espacios adyacentes.\n",
    "- Una de las simplificaciones especificadas en la letra es que se comienza con un tablero con todas sus fichas posicionadas y solo resta moverlas. Para elegir tales tableros iniciales utilizamos una función que los genera aleatoriamente.\n",
    "- Existen fichas negras y rojas, dado que las reglas indican que siempre comienza el jugador con las fichas negras diremos que el jugador que nos interesa entrenar (el jugador 1) es el que juega primero, es decir, el que tiene las fichas negras. Las fichas rojas corresponderán al oponente (el jugador 2).\n",
    "- Definimos como empate si se dan mas de 500 movimientos en total y aún ningún jugador ha ganado.\n",
    "\n",
    "\n",
    "## 2.2 Tablero\n",
    "\n",
    "- Representamos un tablero como una matriz de 5x5 en donde colocamos 1's en las coordenadas que el jugador 1 tiene sus fichas y 2's en las coordenadas donde el jugador 2 tiene sus fichas. El resto de los lugares libres se representan con 0's.\n",
    "- Luego de haber leído el ejemplo del juego de damas quisimos trazar una similitud con el juego en cuestión. Creemos que las características del tablero que elegimos son las más representativas del estado del juego, en el sentido de que con solo listarlas nos da una idea de qué jugador está más cerca de ganar y así poder llegar a una forma correcta de dar valores a los diferentes estados de los tableros. \n",
    "A continuación, una serie de ejemplos:  \n",
    "  \n",
    "$\\begin{pmatrix}\n",
    "0 & 1 & 1 & 1 & 1 \\\\\n",
    "0 & 2 & 0 & 0 & 2 \\\\\n",
    "0 & 2 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 2 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$ $X_{1}=$ 3, $X_{2}=$ 0, $X_{3}=$ 0, $X_{4}=$ 0, $X_{5}=$ 0, $X_{6}=$ 1, $X_{7}=$ 1, $X_{8}=$ 0  \n",
    "\n",
    "$\\begin{pmatrix}\n",
    "0 & 0 & 1 & 0 & 1 \\\\\n",
    "0 & 2 & 2 & 0 & 1 \\\\\n",
    "0 & 2 & 2 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$  $X_{1}=$ 0, $X_{2}=$ 2, $X_{3}=$ 0, $X_{4}=$ 0, $X_{5}=$ 2, $X_{6}=$ 2, $X_{7}=$ 1, $X_{8}=$ 1\n",
    "\n",
    "$\\begin{pmatrix}  \n",
    "1 & 0 & 2 & 2 & 2 \\\\\n",
    "0 & 1 & 0 & 0 & 2 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$ $X_{1}=$ 0, $X_{2}=$ 0, $X_{3}=$ 3, $X_{4}=$ 0, $X_{5}=$ 2, $X_{6}=$ 1, $X_{7}=$ 1, $X_{8}=$ 0  \n",
    "  \n",
    "Estos son ejemplos de tableros finales. En términos generales sabemos que tenemos tableros finales si:\n",
    "- Las adyacencias horizontales son 3 ó\n",
    "- Las adyacencias verticales son 3 ó\n",
    "- Las adyacencias diagonales de izquierda (arriba) a derecha (abajo) son 3 ó\n",
    "- Las adyacencias diagonales de derecha (arriba) a izquierda (abajo) son 3 ó\n",
    "- Las adyacencias horizontales son 2 y las adyacencias verticales son 2\n",
    "\n",
    "\n",
    "## 2.3 Algoritmo\n",
    "\n",
    "### 2.3.1 Visión general\n",
    "\n",
    "Siguiendo la idea planteada en el libro, dividimos el algoritmo en 4 módulos: _Performance System, Critic, Generalizer y Generator_.\n",
    "\n",
    "**Performance System**: Utilizando las funciones aprendidas, es el que se encarga de realizar una partida. Toma como entrada un tablero inicial y su salida corresponde a un historial de juego.\n",
    "Definimos un historial como todos los tableros por los que pasa el jugador 1 luego de que realiza un movimiento.  \n",
    "  \n",
    "**Critic**: Se encarga de producir los ejemplos de entrenamiento. Toma como entrada un historial de juego y su salida es un conjunto de tuplas $<t,V_{train}(t)>$, donde $V_{train}(t)$ es un valor aproximado que calculamos de la siguiente manera:\n",
    "  \n",
    "  $V_{train}(t_{intermedio}) ← V_{op}(t'_{siguiente\\space tablero\\space en\\space el\\space historial})$\n",
    "  \n",
    "  \n",
    "**Generalizer**: Toma como entrada los ejemplos de entrenamiento y su salida es una nueva aproximación de la función objetivo, luego de haber aplicado el algoritmo de mínimos cuadrados. Aquí es donde se ajustan los valores $w_{i}$. El algoritmo de ajuste que utilizamos es el que se dio en clase.\n",
    "  \n",
    "**Generator**: Es el encargado tanto de generar un nuevo tablero inicial, así como de configurar todos los parámetros necesarios para una nueva partida. Toma como parámetro una función $V_{op}$ y su salida es un nuevo tablero inicial.\n",
    "\n",
    "Definimos una **iteración** como una vuelta completa sobre cada uno de los módulos definidos arriba.\n",
    "\n",
    "### 2.3.2 Ajuste de la función\n",
    "\n",
    "El orden para completar un ajuste de la función objetivo sería: _Generator -> Performance System -> Critic -> Generalizer_ y el ajuste se realiza en el módulo _Generalizer_.\n",
    "\n",
    "### 2.3.3 Parametrización\n",
    "\n",
    "A nivel de implementación, decidimos diseñar el código de manera tal que nos resultara fácil ajustar los parámetros para el momento de realizar la experimentación. En dicha sección se prueba con varios valores de μ, en algunos casos se prueba con enfriamiento, se prueba con un factor de descuento, etc.\n",
    "\n",
    "## 2.4 Contrarios\n",
    "\n",
    "- Para generar los tableros al azar se genera una matriz de 5x5 con valores todos en 0. Luego, para poblar el tablero, para cada jugador se generan tuplas que representan las coordenadas de dónde va a ser colocada la ficha. En ambas coordenadas se usa la función de Python randint(), hasta que se encuentra un lugar libre y se coloca allí la ficha.\n",
    "- Para generar los movimientos al azar se generan todos los posibles siguientes tableros, se los guarda en una lista _L_ y se usa la función de Python choice sobre _L_, la cual retorna un elemento al azar.\n",
    "- Los $w_{i}$ se actualizan en cada iteración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbGVdjwFUMZ8"
   },
   "source": [
    "## 3. Experimentación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dw_7cqyfUMZ-"
   },
   "source": [
    "## 3.1 Oponente Aleatorio \n",
    "\n",
    "En esta primera parte mostraremos los resultados obtenidos de jugar con un oponente que siempre juegue de forma aleatoria.\n",
    "En cada punto se jugó un total de 120 partidas, ajustando los $w_{i}$ luego de cada partida.\n",
    "\n",
    "### 3.1.1 $w_{i}$ iniciales\n",
    "\n",
    "Siempre que se comienza a entrenar, se realiza un entrenamiento inicial con tableros finales valorados con 1, 0 o -1. Una vez generado este conjunto de tableros finales creamos un historial y llamamos a los módulos _Critic_ y luego a _Generalizer_. Así, ajustamos tantas veces como tableros finales haya en el conjunto. Finalmente, con la función $V$ y con los  $w_{i}$ conseguidos iniciamos el entrenamiento.\n",
    "     \n",
    "Probamos con tres vectores iniciales (pre-entrenamiento inicial) distintos y nos quedamos con el vector de $w_{i}$ que nos dio un porcentaje más alto de partidas ganadas.\n",
    "Dichos vectores fueron los siguientes:\n",
    "\n",
    "1. (0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25)\n",
    "2. (0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75)\n",
    "3. (0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45)\n",
    "    \n",
    "  Resultados:  \n",
    "<table>\n",
    "  <tr style=\"font-weight:bold\">\n",
    "    <th>(w0,w1,w2,w3,w4,w5,w6,w7,w8)</th>\n",
    "    <th>Partidas ganadas</th>\n",
    "    <th>% de partidas ganadas</th>\n",
    "    <th>% de partidas empatadas</th>\n",
    "  </tr>   \n",
    "  <tr>\n",
    "    <td>(0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25)</td>\n",
    "    <td>91</td>\n",
    "    <td>50.5</td>\n",
    "    <td>13.3</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75)</td>\n",
    "    <td>76</td>\n",
    "    <td>42.2</td>\n",
    "    <td>15.6</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45)</td>\n",
    "    <td>106</td>\n",
    "    <td>58.8</td>\n",
    "    <td>13.8</td>\n",
    "  </tr>    \n",
    "</table>\n",
    "\n",
    "En conclusión, nos quedamos con el vector inicial $W$ igual a 0.45 en todas sus entradas.\n",
    "  \n",
    "### 3.1.2 Elección del próximo movimiento\n",
    "En todos los experimentos que realizamos en esta sección, el jugador dado el conjunto de todos los movimientos posibles elige el que tiene una valoración mayor según la función objetivo $V$ que conoce hasta el momento.  \n",
    "\n",
    "### 3.1.3 Aproximación para $𝑉_{𝑡𝑟𝑎𝑖𝑛}(𝑡)$\n",
    "\n",
    "Si bien al principio la idea fue mapear el espacio de los posibles tableros a un entorno (-1, 1) donde los tableros ganadores tomaran el valor 1 y los perdedores el valor -1, esto no se pudo lograr de forma estricta, dado que los $w_{i}$ tendían a crecer y las características que decidimos representar de los tableros podían llegar a ser números que hicieran crecer la función, haciendo que tomara valores más grandes que 1.\n",
    "\n",
    "Además, de alguna manera, imponer que los tableros finales tuvieran valores -1 o 1 despistaba al algoritmo. Creemos que puede haber sido por tener tableros anteriores con valores más (menos) grandes que un tablero final ganador (perdedor). Lo que hicimos para enfrentar este problema fue directamente descartar los tableros finales al momento de generar el conjunto de entrenamiento. \n",
    "\n",
    "Otra opción podría haber sido utilizar la noción de que dado un $t\\space/\\space t\\space es \\space final =>$ $V_{train}(t)=V_{op}(t)$ logrando así mantener la idea de que los tableros con valores mas grandes son mejores.\n",
    "\n",
    "### 3.1.4 Movimientos aleatorios\n",
    "Uno de los fenómenos que observamos es que luego de muchos entrenamientos el algoritmo se estancaba y no exporaba nuevos tableros. Decidimos probar tomando cada X movimientos un movimiento aleatorio y ver qué sucedía con el porcentaje de partidas gandas. Este fue el resultado:\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Cada 2 movimientos uno aleatorio</th>\n",
    "    <th>Cada 3 movimientos uno aleatorio</th>\n",
    "    <th>Cada 4 movimientos uno aleatorio</th>\n",
    "    <th>Cada 5 movimientos uno aleatorio</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>77.7%</td>\n",
    "    <td>87.6%</td>\n",
    "    <td>80.5%</td>\n",
    "    <td>83.3%</td>\n",
    "  </tr>    \n",
    "</table>\n",
    "  \n",
    "  En conclusión, nos quedaremos con la configuración que nos dio un total de 86.6% de partidas ganadas. Es decir, elegir un movimiento aleatorio cada 3 ò 5 movimientos (contando solo movimientos del Jugador que está siendo entrenado).\n",
    "  \n",
    "  _**Observación:** Notamos que luego de fijar esta variante no tenemos empates._\n",
    "  \n",
    "### 3.1.5 Tasa de aprendizaje μ y enfriamiento\n",
    "A nivel conceptual, a mayor valor de μ, mayor es el ajuste. Fuimos probando con distintos valores de μ en cada entrenamiento, y los resultados fueron los siguientes:\n",
    "    \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>μ</th>\n",
    "    <th>% partidas ganadas</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>μ=0.1</td>\n",
    "    <td>91.3</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>μ=0.01</td>\n",
    "    <td>94.4</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>μ=0.001</td>\n",
    "    <td>95.6</td>\n",
    "  </tr>  \n",
    "</table>\n",
    "\n",
    "En un intento por mejorar aún más el porcentaje probamos con distintas tasas de enfriamiento, pero no vimos cambios notorios en los resultados finales.\n",
    "\n",
    "En conclusión, nos quedamos con μ fijo igual a 0.001.\n",
    "\n",
    "\n",
    "### 3.1.6 Factor de descuento $\\gamma$\n",
    "Definimos un factor de descuento igual a $(0.9)^k$ con el objetivo de dar una menor valoración a los tableros que estan más cerca del tablero inicial.\n",
    "\n",
    "El porcentaje de partidas ganadas sin el factor de descuento fue de 95.6%.\n",
    "\n",
    "Con el factor de descuento fue de 96.4%, por lo que decidimos conservarlo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gt0G9k9UMaB"
   },
   "source": [
    "## 3.2 Oponente inteligente\n",
    "\n",
    "En esta sección veremos los resultados obtenidos al jugar contra un oponente que es una version anterior del jugador 1.\n",
    "\n",
    "### 3.2.1 Oponente con pesos fijos\n",
    " \n",
    "Primero probamos entrenar contra un jugador 2 que utiliza la función objetivo $V_{op}$ parte 3.1 y nunca la actualiza:\n",
    "\n",
    "Aquí notamos que con los hiperparámentros utilizados en la parte 3.1 el jugador 1 sigue teniendo mejor porcentaje de partidas ganadas. Pero para estar seguros que no existia una mejor combinacion de hiperparametros y variables realizamos el siguiente experimento:\n",
    "- Aumentamos la cantidad de partidas jugadas a 1000.   \n",
    "- Probamos con distintos factores de aprendizaje.  \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>μ = 0.1</th>\n",
    "    <th>μ = 0.01</th>\n",
    "    <th>μ = 0.001</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>jugador 1 43.3% / juegador 2  37.8%  </td>\n",
    "    <td>jugador 1 49.6% / juegador 2 34.6</td>\n",
    "    <td>jugador 1 72.7% / juegador 2 27.3%</td>  \n",
    "  </tr>\n",
    "     <caption>Esta tabla representa el porcentaje de partidas ganadas de cada jugador  jugador 1/jugador 2 variando μ </caption>\n",
    "</table> \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QMMPschVUMaH"
   },
   "source": [
    "### 3.2.2 Oponente actualizado\n",
    "\n",
    "Luego, tomamos la decisión de ir variando los pesos del jugador 2 cada vez que transcurrían $x$ partidas. Los valores $x$ que probamos son los siguientes:\n",
    "1. 50 partidas\n",
    "2. 20 partidas\n",
    "3. 5 partidas\n",
    "4. 2 partidas\n",
    "\n",
    "Utilizamos un factor de μ = 0.001 para todos los casos, e inicializamos los pesos de ambos jugadores en 0.45 en todas sus entradas.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Cada 50 partidas</th>\n",
    "    <th>Cada 20 partidas</th>\n",
    "    <th>Cada 5 partidas</th>\n",
    "    <th>Cada 2 partidas</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>70.0% / 30.0% </td>\n",
    "    <td>70.5% / 29.5%</td>\n",
    "    <td>69.1% / 30.9%</td>\n",
    "    <td>68.2% / 31.8%</td>  \n",
    "  </tr>\n",
    "     <caption>Esta tabla muestra el porcentaje de partidas ganadas de jugador 1/jugador 2 variando $x$ </caption>\n",
    "</table>\n",
    "\n",
    "**Observación:** No hubieron empates en esta parte.\n",
    "\n",
    "Se puede ver que no hubo un cambio significativo a medida que variamos $x$ si comparamos los porcentajes de ambos jugadores.\n",
    "Inicialmente suponíamos que a medida que el Jugador menos inteligente, en este caso el Jugador 2, actualizaba sus pesos más frecuentemente, ibamos a observer cambios en los porcentajes, y obtener mejoras en la cantidad de partidas ganadas por el Jugador 2.\n",
    "Pero asumimos que el Jugador 1, al estar entrenando de forma continua, siempre se encuentra en mejores condiciones para llevarse la partida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GBheaaF8UMaI"
   },
   "source": [
    "## 3.3 Competencia\n",
    "\n",
    "En esta sección, la idea es que compitan la versión del jugador 1 entrenado en la primer parte de la tarea, vs la versión del jugador 1 entrenado en la segunda parte, esto es el jugador que entrenó vs un oponente que accionaba de forma aleatoria, compitiendo frente a el jugador que entrenó vs una versión anterior de él mismo.\n",
    "\n",
    "Llamaremos Jugador-Random al que entrenó vs un oponente random, y Jugador-Inteligente al restante.\n",
    "\n",
    "Como vimos en partes anteriores, obtuvimos mejores dividendos para el Jugador-Random cuando se jugaba con un mu pequeño, por ejemplo con mu = 0.00001. \n",
    "Para el Jugador-Inteligente, la idea es escoger al jugador que entrenó vs el mejor oponente posible.\n",
    "\n",
    "Dividiremos esta sección en 3 competencias distintas.\n",
    "\n",
    "### 3.3.1 Competencia 1:\n",
    "- Tomaremos los pesos de la parte 1, resultantes de entrenar al Jugador-Random con pesos iniciales en 0.45 y μ = 0.00001, donde se alcanzaban aproximadamente 95% de partidas ganadas.\n",
    "Los mismos son:   \n",
    "   \n",
    "                                  [0.308,0.394,0.394,0.408,0.408,0.430,0.429,0.433,0.434]\n",
    "Consideramos estos pesos los más \"exigentes\" contra los que podría competir el Jugador-Inteligente entrenado en la segunda parte.\n",
    "\n",
    "- Partiendo de los mismos pesos iniciales (con todas sus coordenadas con el valor 0.45) y el mismo valor de μ (0.00001), los pesos resultantes de entrenar al jugador de la parte 2 contra un oponente inteligente (que era la versión del jugador pero 5 partidas atrás) fueron los siguientes:   \n",
    "   \n",
    "                                 [0.298,0.403,0.403,0.411,0.411,0.387,0.386,0.401,0.401].\n",
    "\n",
    "- Obtuvimos los siguientes resultados al efectuar esta competencia:\n",
    "Jugador-Random ganó el 22.20% del total de las 1000 partidas.\n",
    "Jugador-Inteligente ganó el 24.59% del total de las 1000 partidas.\n",
    "Las partidas no finalizaron con un ganador en el 53.2% de las partidas.  \n",
    "  \n",
    "    \n",
    "  \n",
    "    \n",
    "Para no quedarnos solamente con los datos anteriores, probamos utilizar otras combinaciones de Jugador-Random/Jugador-Inteligente.\n",
    "En este caso, fijamos los pesos del Jugador-Random:    \n",
    " \n",
    "                                    [-0.256,0.082,0.082,0.137,0.124,0.207,0.203,0.224,0.230]. \n",
    "\n",
    "Estos son los resultantes de hacerlo competir en la primer parte de la tarea con un valor de μ  un poco mas grande, 0.001.\n",
    "\n",
    "###3.3.2 Competencia 2:\n",
    "\n",
    "- Probamos con 2 \"jugadores inteligentes\" generados en la segunda parte de esta tarea (siempre partiendo de los mismos pesos iniciales 0.45, y jugando contra una versión 5 partidas atrasada).\n",
    "\n",
    " Para el primer Jugador-Inteligente utilizamos los mismos valores que en la competencia 1, estos son:  \n",
    "                         [0.298,0.403,0.403,0.411,0.411,0.387,0.386,0.401,0.401].\n",
    "\n",
    "- El resultado de esta competencia fue el siguiente:\n",
    "\n",
    " Jugador-Random ganó el 21.6% del total de las 1000 partidas.\n",
    "Jugador-Inteligente ganó el 25.7% del total de las 1000 partidas.\n",
    "Las partidas no finalizaron con un ganador en el 52.7% de las partidas.\n",
    "\n",
    "  Aquí notamos que el Jugador-Inteligente obtiene mejores resultados que en la primer competencia. \n",
    "\n",
    "##3.3.3 Competencia 3:\n",
    "\n",
    "- Para el segundo Jugador-Inteligente utilizamos mu = 0.001, obteniendo los siguientes pesos finales entrenado vs su versión anterior:  \n",
    "\n",
    "                        [-0.291,0.085,0.085,0.149,0.129,0.077,0.077,0.115,0.121]\n",
    "\n",
    "- Al realizar nuevamente la competencia, los resultados obtenidos fueron un poco inferiores:\n",
    "\n",
    "  Jugador-Random ganó el 21.7% del total de las 1000 partidas.\n",
    "Jugador-Inteligente ganó el 24.5% del total de las 1000 partidas.\n",
    "Las partidas no finalizaron con un ganador en el 53.8% de las partidas.  \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "Analizando los resultados obtenidos, podemos decir que si bien la comptenecia es muy pareja, se encuentra mejor entrenado el Jugador-Inteligente, y consigue ganar más partidas que su oponente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vq9y7P9JUMaU"
   },
   "source": [
    "## 4. Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVpx88G8UMaV"
   },
   "source": [
    "- Los mejores resultados lógicamente se dieron cuando jugabamos con un oponente aleatorio, dado que la inteligencia de tal era nula. A medida que el oponente se volvía más inteligente el porcentaje de partidas ganadas decreció notoriamente. En este punto también observamos que con un oponente aleatorio casi no habían empates.\n",
    "- Otro punto a destacar, cuando oponente es aleatorio el hecho de que usar enfriamiento en la tasa de aprendizaje casi no cambió los resultados, con un μ =0.001 que es bastante pequeño obtuvimos buenos resultados. Lo que significa que si desde un principio limitábamos la variación de el vector $W$ podíamos converger a valores satisfactorios. Esto ocurre también con esto cuando el oponente es una versión anterior del jugador 1.\n",
    "- También notamos que cuando entrenamos contra un oponente random, en un momento el porcentaje de partidas ganadas se estanca a pesar de jugar más partidas. Esto pudimos solucionarlo haciendo que el jugador 1 cada cierta cantidad de movimientos realice uno aleatorio, permitiendo así que explore nuevos caminos y redujimos mucho más los empates.\n",
    "En el caso de el oponente aleatorio reducimos a cero los empates pero en el caso del oponente inteligente no fue así, con valores más altos de μ volvían a aparecer los empates. Con lo que concluimos que es la combinación de un factor de aprendizaje pequeño más los movimientos aleatorios son los que realmente hacen decrecer los empates.\n",
    "- Algo que creemos que mejoraría el algoritmo es poder normalizar los atributos y los $w_{i}$ de forma tal de poder lograr que la imagen de la función $V_{op}$ realemnte se encuentre en el intervalos $[-1,1]$. Logrando así poder darle valores conocidos a tableros finales. \n",
    "  Inicialmente creimos conveniente, para tener una idea de qué valores $w_{i}$ para comenzar a jugar podían ser útiles, crear historiales de juego que constaran de solo un tablero final, dado que en principio $V_{train}$ para esos tableros era conocida, pero con el avance de la tarea nos dimos cuenta que era una falacia dado que existian tableros intemedios con un valor de $V_{op}$ mayor que 1.  \n",
    "- También podría mejorar el algoritmo cambiar la forma en la que elegimos el próximo movimiento. La forma que nosotros aplicamos si bien nos fue útil, no creemos que sea la mejor. Una alternativa a esto es dado el conjunto de movimientos posibles para el jugador 1 si suponemos que nuestro oponente se mueve de forma óptima (segun la $V_{op}$ conocida hasta el momento) podríamos quedarnos con el movimiento que lleve al oponente al tablero menor valorado para él.\n",
    "- Algo que creemos que también mejoraría el algoritmo es cambiar agunas de las características que elegimos representar en el tablero, dado que podemos tener 3 adyacencias diagonales dándole un valor alto a su $X_{i}$  correspondiente. Pero si las fichas estan en una diagonal de 3 el jugador nunca podrá completar esta diagonal y ganar. Un ejemplo de tablero es el siguiente:  \n",
    "$\\begin{pmatrix}  \n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$. \n",
    "  \n",
    "  Una solución es chequear en qué coordenadas de la matriz se está formando la diagonal para no permitir que el jugador que nos interesa entrenar coloque sus fichas de esta forma.\n",
    "\n",
    "- Otro aporte que podría ser de utilidad para que nuestro algoritmo mejore, además de fijarnos en el valor de nuestros atributos, sería verificar si el oponente tiene posibilidad de ganar en su próximo movimiento (por tener por ejemplo 3 fichas adyacentes) y entonces colocar una de nuestras fichas en ese lugar, de manera de impedir su victoria."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AA_Obligatorio1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
